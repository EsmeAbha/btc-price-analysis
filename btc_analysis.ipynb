{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytWY0O46bNsT",
        "outputId": "1b3fe41c-19c9-4404-9359-52918cb84399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = '/content/drive/MyDrive/bitcoin/BTC-USD.csv'\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "total_rows = len(df)\n",
        "train_rows = int(0.7 * total_rows)\n",
        "\n",
        "\n",
        "train_df = df.iloc[:train_rows]\n",
        "val_df = df.iloc[train_rows:]\n",
        "\n",
        "train_df.to_csv('training_data.csv', index=False)\n",
        "val_df.to_csv('validation_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "fzVUB9obcAQF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)\n",
        "print(train_df)\n",
        "print(val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxUfev38d1FL",
        "outputId": "a2695ff9-0eda-4d66-92fd-b91a0b698b8c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Date          Open          High           Low         Close  \\\n",
            "0     2014-09-17    465.864014    468.174011    452.421997    457.334015   \n",
            "1     2014-09-18    456.859985    456.859985    413.104004    424.440002   \n",
            "2     2014-09-19    424.102997    427.834991    384.532013    394.795990   \n",
            "3     2014-09-20    394.673004    423.295990    389.882996    408.903992   \n",
            "4     2014-09-21    408.084991    412.425995    393.181000    398.821014   \n",
            "...          ...           ...           ...           ...           ...   \n",
            "3515  2024-05-02  58253.703125  59602.296875  56937.203125  59123.433594   \n",
            "3516  2024-05-03  59122.300781  63320.503906  58848.312500  62889.835938   \n",
            "3517  2024-05-04  62891.031250  64494.957031  62599.351563  63891.472656   \n",
            "3518  2024-05-05  63892.453125  64610.890625  62955.304688  64031.132813   \n",
            "3519  2024-05-06  64032.632813  65425.679688  63750.796875  63750.796875   \n",
            "\n",
            "         Adj Close       Volume  \n",
            "0       457.334015     21056800  \n",
            "1       424.440002     34483200  \n",
            "2       394.795990     37919700  \n",
            "3       408.903992     36863600  \n",
            "4       398.821014     26580100  \n",
            "...            ...          ...  \n",
            "3515  59123.433594  32711813559  \n",
            "3516  62889.835938  33172023048  \n",
            "3517  63891.472656  20620477992  \n",
            "3518  64031.132813  18296164805  \n",
            "3519  63750.796875  22949939200  \n",
            "\n",
            "[3520 rows x 7 columns]\n",
            "            Date          Open          High           Low         Close  \\\n",
            "0     2014-09-17    465.864014    468.174011    452.421997    457.334015   \n",
            "1     2014-09-18    456.859985    456.859985    413.104004    424.440002   \n",
            "2     2014-09-19    424.102997    427.834991    384.532013    394.795990   \n",
            "3     2014-09-20    394.673004    423.295990    389.882996    408.903992   \n",
            "4     2014-09-21    408.084991    412.425995    393.181000    398.821014   \n",
            "...          ...           ...           ...           ...           ...   \n",
            "2459  2021-06-11  36697.031250  37608.695313  36044.449219  37334.398438   \n",
            "2460  2021-06-12  37340.144531  37408.925781  34728.191406  35552.515625   \n",
            "2461  2021-06-13  35555.789063  39322.781250  34864.109375  39097.859375   \n",
            "2462  2021-06-14  39016.968750  40978.363281  38757.285156  40218.476563   \n",
            "2463  2021-06-15  40427.167969  41295.269531  39609.468750  40406.269531   \n",
            "\n",
            "         Adj Close       Volume  \n",
            "0       457.334015     21056800  \n",
            "1       424.440002     34483200  \n",
            "2       394.795990     37919700  \n",
            "3       408.903992     36863600  \n",
            "4       398.821014     26580100  \n",
            "...            ...          ...  \n",
            "2459  37334.398438  38699736985  \n",
            "2460  35552.515625  37924228550  \n",
            "2461  39097.859375  40669112838  \n",
            "2462  40218.476563  43148914673  \n",
            "2463  40406.269531  46420149185  \n",
            "\n",
            "[2464 rows x 7 columns]\n",
            "            Date          Open          High           Low         Close  \\\n",
            "2464  2021-06-16  40168.691406  40516.777344  38176.035156  38347.062500   \n",
            "2465  2021-06-17  38341.421875  39513.671875  37439.675781  38053.503906   \n",
            "2466  2021-06-18  38099.476563  38187.261719  35255.855469  35787.246094   \n",
            "2467  2021-06-19  35854.527344  36457.796875  34933.062500  35615.871094   \n",
            "2468  2021-06-20  35563.140625  36059.484375  33432.074219  35698.296875   \n",
            "...          ...           ...           ...           ...           ...   \n",
            "3515  2024-05-02  58253.703125  59602.296875  56937.203125  59123.433594   \n",
            "3516  2024-05-03  59122.300781  63320.503906  58848.312500  62889.835938   \n",
            "3517  2024-05-04  62891.031250  64494.957031  62599.351563  63891.472656   \n",
            "3518  2024-05-05  63892.453125  64610.890625  62955.304688  64031.132813   \n",
            "3519  2024-05-06  64032.632813  65425.679688  63750.796875  63750.796875   \n",
            "\n",
            "         Adj Close       Volume  \n",
            "2464  38347.062500  39211635100  \n",
            "2465  38053.503906  37096670047  \n",
            "2466  35787.246094  36200887275  \n",
            "2467  35615.871094  31207279719  \n",
            "2468  35698.296875  36664034054  \n",
            "...            ...          ...  \n",
            "3515  59123.433594  32711813559  \n",
            "3516  62889.835938  33172023048  \n",
            "3517  63891.472656  20620477992  \n",
            "3518  64031.132813  18296164805  \n",
            "3519  63750.796875  22949939200  \n",
            "\n",
            "[1056 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "\n",
        "\n",
        "df.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "X = df[['Year','Month','Day','Open', 'Low','Close','Adj Close', 'Volume']]\n",
        "y = df['High']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "train_loss = model.evaluate(X_train_scaled, y_train)\n",
        "test_loss = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Train Loss:\", train_loss)\n",
        "print(\"Test Loss:\", test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4remZfPzgkJK",
        "outputId": "e9354ed2-a468-44e8-b7d4-f3d20ffeba13"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "71/71 [==============================] - 2s 8ms/step - loss: 609442048.0000 - val_loss: 582841600.0000\n",
            "Epoch 2/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 427930272.0000 - val_loss: 150464448.0000\n",
            "Epoch 3/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 116407288.0000 - val_loss: 76509696.0000\n",
            "Epoch 4/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 54102300.0000 - val_loss: 28368328.0000\n",
            "Epoch 5/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 17254348.0000 - val_loss: 6950484.0000\n",
            "Epoch 6/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 4518367.5000 - val_loss: 2086833.2500\n",
            "Epoch 7/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 1882723.6250 - val_loss: 1258319.1250\n",
            "Epoch 8/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 1352150.5000 - val_loss: 1031727.4375\n",
            "Epoch 9/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 1102959.7500 - val_loss: 804807.8125\n",
            "Epoch 10/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 916249.6250 - val_loss: 662363.6250\n",
            "Epoch 11/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 777407.6250 - val_loss: 560288.6250\n",
            "Epoch 12/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 673137.0625 - val_loss: 471818.9375\n",
            "Epoch 13/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 586721.6875 - val_loss: 405544.3438\n",
            "Epoch 14/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 519736.9375 - val_loss: 356273.1875\n",
            "Epoch 15/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 463961.2500 - val_loss: 322492.4688\n",
            "Epoch 16/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 429585.5000 - val_loss: 309586.0000\n",
            "Epoch 17/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 391441.0938 - val_loss: 265978.8438\n",
            "Epoch 18/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 368532.5625 - val_loss: 281978.3750\n",
            "Epoch 19/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 350925.9375 - val_loss: 231726.9688\n",
            "Epoch 20/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 329372.7500 - val_loss: 242309.0156\n",
            "Epoch 21/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 313741.8750 - val_loss: 209876.9219\n",
            "Epoch 22/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 303081.6250 - val_loss: 216014.7188\n",
            "Epoch 23/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 289562.5000 - val_loss: 189650.9219\n",
            "Epoch 24/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 274883.6875 - val_loss: 182846.3906\n",
            "Epoch 25/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 269349.2812 - val_loss: 176821.9688\n",
            "Epoch 26/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 258522.1250 - val_loss: 170973.9062\n",
            "Epoch 27/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 250579.1250 - val_loss: 181311.2500\n",
            "Epoch 28/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 245863.7344 - val_loss: 160104.0781\n",
            "Epoch 29/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 242390.9062 - val_loss: 154536.9844\n",
            "Epoch 30/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 227626.5781 - val_loss: 195593.5312\n",
            "Epoch 31/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 227382.4062 - val_loss: 156529.5938\n",
            "Epoch 32/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 226402.3906 - val_loss: 145093.0625\n",
            "Epoch 33/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 220193.5000 - val_loss: 145618.6562\n",
            "Epoch 34/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 211503.3750 - val_loss: 138583.3594\n",
            "Epoch 35/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 204517.5156 - val_loss: 142776.1719\n",
            "Epoch 36/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 205666.9062 - val_loss: 143341.1250\n",
            "Epoch 37/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 197439.6875 - val_loss: 130529.0781\n",
            "Epoch 38/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 197551.4375 - val_loss: 131159.2344\n",
            "Epoch 39/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 192633.2500 - val_loss: 130421.8750\n",
            "Epoch 40/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 186660.8594 - val_loss: 125334.1719\n",
            "Epoch 41/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 184543.9219 - val_loss: 129820.6953\n",
            "Epoch 42/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 184050.0312 - val_loss: 124392.0156\n",
            "Epoch 43/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 188064.2500 - val_loss: 121896.0156\n",
            "Epoch 44/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 190684.7344 - val_loss: 123894.3516\n",
            "Epoch 45/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 179089.4688 - val_loss: 140937.7344\n",
            "Epoch 46/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 206190.8750 - val_loss: 182805.1875\n",
            "Epoch 47/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 184762.9531 - val_loss: 121112.7109\n",
            "Epoch 48/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 177903.7188 - val_loss: 149848.2969\n",
            "Epoch 49/100\n",
            "71/71 [==============================] - 0s 6ms/step - loss: 186520.6406 - val_loss: 168234.6875\n",
            "Epoch 50/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 187079.3750 - val_loss: 141158.3750\n",
            "Epoch 51/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 182851.1406 - val_loss: 117678.5156\n",
            "Epoch 52/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 174990.0000 - val_loss: 140027.2969\n",
            "Epoch 53/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 175833.2656 - val_loss: 118075.9141\n",
            "Epoch 54/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 168040.1094 - val_loss: 118524.8047\n",
            "Epoch 55/100\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 180158.0312 - val_loss: 118008.0391\n",
            "Epoch 56/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 185425.9688 - val_loss: 115162.5781\n",
            "Epoch 57/100\n",
            "71/71 [==============================] - 0s 6ms/step - loss: 172843.5156 - val_loss: 114572.8516\n",
            "Epoch 58/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 175104.1094 - val_loss: 115012.1094\n",
            "Epoch 59/100\n",
            "71/71 [==============================] - 0s 5ms/step - loss: 173534.3125 - val_loss: 143964.6562\n",
            "Epoch 60/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 180079.5625 - val_loss: 117915.8438\n",
            "Epoch 61/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 172295.8125 - val_loss: 144875.7031\n",
            "Epoch 62/100\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 185126.0469 - val_loss: 181342.5312\n",
            "88/88 [==============================] - 0s 2ms/step - loss: 151443.7031\n",
            "22/22 [==============================] - 0s 2ms/step - loss: 112937.2422\n",
            "Train Loss: 151443.703125\n",
            "Test Loss: 112937.2421875\n"
          ]
        }
      ]
    }
  ]
}
